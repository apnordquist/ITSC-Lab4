from secml.ml.peval.metrics import CMetric
from secml.ml.peval.metrics import CMetricAccuracy
from secml.ml.kernels import CKernelRBF
from secml.ml.classifiers import CClassifierSVM
from secml.array import CArray
from sklearn.preprocessing import MaxAbsScaler
from secml.data.splitter import CTrainTestSplit
import numpy as np
import re
import secml
from secml import settings
from secml.utils import fm
from secml.utils.download_utils import dl_file_gitlab
from secml.utils import pickle_utils

# code provided in lab
repo_url = "https://gitlab.com/secml/secml-zoo"
file_name = "drebin-reduced.tar.gz"
file_path = "datasets/DrebinRed/" + file_name

output_dir = fm.join(settings.SECML_DS_DIR, "drebin-red")
md5_digest = "ecf87ddedf614dd53b89285c29cf1caf"

ds_path = fm.join(output_dir, file_name)

if not fm.file_exist(ds_path):
    try:
        min_version = re.search(r'\d+\.\d+(?:\.\d+)?',
                                secml.__version__).group(0)
        dl_file_gitlab(repo_url, file_path, output_dir,
                       branch='v' + min_version, md5_digest=md5_digest)
    except Exception as e:
        dl_file_gitlab(repo_url, file_path, output_dir, md5_digest=md5_digest)
ds = pickle_utils.load(ds_path)

# dataset checks

print("Total samples:", ds.num_samples)

y = ds.Y.tondarray().flatten()  # need to convert for sorting
unique, counts = np.unique(y, return_counts=True)
print(f"Benign (0): {counts[0]}, Malicious (1): {counts[1]}")

print("Number of features:", ds.num_features)


# part 2 - Data Preprocessing and Splitting
print("Processing and Splitting Data")

splitter = CTrainTestSplit(train_size=0.8)  # set training for 80%
training, testing = splitter.split(ds)

print(f"Training samples: {training.num_samples}")
print(f"Testing samples: {testing.num_samples}")

print("Normalizing Data")

# convert to sparse format
X_train = training.X.tosparse().get_data()
X_test = testing.X.tosparse().get_data()

# needed alternative method as SecML normalizer is memory intensive
scaler = MaxAbsScaler().fit(X_train)
training.X = CArray(scaler.transform(X_train))
testing.X = CArray(scaler.transform(X_test))

# print normalized samples


def print_class_distribution(dataset, name):
    y = dataset.Y.tondarray().flatten()
    unique, counts = np.unique(y, return_counts=True)
    print(f"{name} - Benign (0): {counts[0]}, Malicious (1): {counts[1]}")


print_class_distribution(training, "Training set")
print_class_distribution(testing, "Testing set")


# part 3 - Classifier Training
print("Training Classifier")

# create the support vector machine (SVM)
svm = CClassifierSVM(kernel=CKernelRBF(), C=1.0)  # default

# start training
svm.fit(training.X, training.Y)
print("Training Completed!")

# get accuracy
y_prediction = svm.predict(training.X)
CMaccuracy = CMetricAccuracy()
training_accuracy = CMaccuracy.performance_score(training.Y, y_prediction)
print(f"\nTraining Accuracy: {training_accuracy:.2%}")


# part 4 - Model Evaluation
print("Evaluating Model")

# get predictions for evaluation
y_true = testing.Y.get_data().ravel()
y_score = svm.predict(testing.X).ravel()

# calculate detection rate
sorted_index = np.argsort(-y_score)
y_sorted = y_true[sorted_index]

total_benign = np.sum(y_true == 0)
total_malicious = np.sum(y_true == 1)

cumulative_benign = np.cumsum(y_sorted == 0)
cumulative_malicious = np.cumsum(y_sorted == 1)

fpr = cumulative_benign / total_benign  # false positive rate
tpr = cumulative_malicious / total_malicious  # true positive rate

# calculate accuracy
if y_prediction.shape[0] != testing.Y.shape[0]:
    y_prediction = y_prediction.reshape((testing.num_samples, -1))[:, 0]

# metrics from the lab
print(f"Accuracy: {CMaccuracy.performance_score(testing.Y, y_prediction):.2%}")
print(f"F1 Score: {CMaccuracy.performance_score(testing.Y, y_prediction):.4f}")

# calculate detection rate
tresh_index = np.argmax(fpr > 0.01) - 1
detection_rate = tpr[tresh_index] if tresh_index >= 0 else 0.0
print(f"\nDetection Rate @ 1% FPR: {detection_rate:.2%}")


# create a confusion matrix
print("\nConfusion Matrix:")
print(CMetric.create('confusion_matrix').performance(testing.Y, y_prediction))
